<div align="center">
<figure>

 <h1> <b>Landscape Generation using Generative Adversarial Networks: A Comparison </b> </h1>
 <h3> Minh Hoang & Kenneth Ma </h3>
 
</figure>
</div>
 <em>Published on December 13, 2022</em>
<div align="right">
 
</div>

## <ins><b> Abstract </b></ins>
In this project, we explore various image generation techniques using Generative Adversarial Networks (GANs) to generate fake images of landscapes. We generated images with 3 different GANs and compared the results. We began by training an existing deep convolution GAN architecture on a landscape image dataset from Kaggle, downsampling the dataset to 64x64 images and adapting the network to reduce overall training time. Then, we extended this DCGAN networks to handle 128x128 and 256x256 images. Since we didn't have time ot train on 256x256 images, we compared our results to 256x256 images generated by a StyleGANv3 that was pre-trained on a different landscape dataset. For an more in-depth overview of our procedure and results, see the following video summary.

Video
[INSERT HERE]().

## <ins><b> Introduction and Problem Statement </b></ins>
Landscapes have always been a source of inspiration to anyone fond of hiking and sightseeing. We wanted to see whether deep learning models trained on landscape images could generate novel landscapes that are as realistic as nature. We began by considering a few types of deep learning models including diffusions models and variational autoencoders, but after learning about Generative Adversarial Networks in class, we became interested in the idea of pitting two models against each other. For this project, we decided to focus on GANs.

## <ins><b> Related Works </b></ins>
We found a few GANs that people were already using to generate landscapes ([Landscape GAN](https://github.com/ryanzhang22/Landscape-GAN?fbclid=IwAR28WStPjOQCleqOePywSS_O6EEZYbFnWxjsO3SzQBH8dNWp5jTnhOLwWns), [GANscapes](https://github.com/robgon-art/GANscapes?fbclid=IwAR3sP2HWann9MNd-n3SFvV4xMWsZrPc02SPDERuFT4S5EQWVSCOUUaFqoXw)). Ultimately, we decided to use a GAN that wasn’t already pre-trained on landscapes. We based our network architecture off of Natsu6767’s Deep Convolutional GAN (originally trained on celebrity faces) and trained it on the [Landscape Pictures](https://www.kaggle.com/datasets/arnaud58/landscape-pictures) dataset by arnaud58 on Kaggle. For the StyleGAN, we found [StyleGanV3](https://github.com/justinpinkney/awesome-pretrained-stylegan3) by justinpinkney on Github that was pre-trained on LHQ-256 (a 256x256 landscapes dataset). Coincidentally, LHQ was one of the datasets we considered early on before we decided to go with the one by arnaud58.

<div align="center">
<figure>
 <img alt="sample1" src="https://raw.githubusercontent.com/hoanganhminh01/Landscape-Generation-GAN/main/data_preprocessed_256/preprocessed_256/00000000_(5).jpg"> 
 <img alt="sample2" src="https://raw.githubusercontent.com/hoanganhminh01/Landscape-Generation-GAN/main/data_preprocessed_256/preprocessed_256/00000023_(7).jpg">
 <img alt="sample3" src="https://raw.githubusercontent.com/hoanganhminh01/Landscape-Generation-GAN/main/data_preprocessed_256/preprocessed_256/00000038_(3).jpg">
</figure>
  *256x256 preprocessed images from "Landscape Pictures" by arnaud58*
</div>

## <ins><b> Methodology </b></ins>
The Landscape Pictures dataset consisted of 4,319 images of landscapes without any metadata. Since the image sizes in the dataset were inconsistent, we needed to crop and resize the images into specific dimensions. We preprocessed the dataset into 3 different dimensions: 64 x 64, 128 x 128 and 256 x 256.

We then adapted Natsu's DCGAN architecture to create the following 3 DCGAN's a trained one each on the 3 preprocessed sizes.
[Insert GAN architectures here]

## <ins><b> Experiments </b></ins>
<div align="center">
<figure>

* DCGAN trained on 64x64 downsampled Landscape Pictures dataset
 * 2000 epochs in 2.5 hrs (avg of 4- 5 sec/epoch)
* DCGAN trained on 128x128 images downsampled Landscape Pictures dataset
 * 1000 epochs in 8 hrs (avg of 27-30 sec/epoch)
* DCGAN trained on 256x256 images downsampled Landscape Pictures dataset
 * Unable to train: CUDA out of Memory error
* Generated images from pre-trained StyleGAN on LHQ-256 dataset
 
 <img alt="model1" src="https://raw.githubusercontent.com/hoanganhminh01/Landscape-Generation-GAN/main/outputs/dcgan.png"> 
 
</figure>
</div>

### <ins><b> Model Evaluation </b></ins>
Since our experiments do not control for variables (we use different model architectures trained on different datasets for different amounts of time) the goal of our project is less about comparing specific training methods and more about conducting an exploration of various GAN networks to generate the “best-looking” landscape images possible. We acknowledge that “best-looking” is quite subjective. Therefore, we’ll showcase our results by making side-by-side comparisons between images generated by different networks and describe any patterns observed.

## <ins><b> Results </b></ins>
### <ins><b> Model Performance </b></ins>
<div align="center">
<figure>

 <img alt="loss1" src="https://raw.githubusercontent.com/hoanganhminh01/Landscape-Generation-GAN/main/outputs/loss64.png"> 
 <img alt="loss2" src="https://raw.githubusercontent.com/hoanganhminh01/Landscape-Generation-GAN/main/outputs/loss128.png">
 
</figure>
</div>

### <ins><b> Results </b></ins>

### <ins><b> Additional Examples: Pre-trained StyleGANv3 </b></ins>

## <ins><b> Conclusion and Future Works </b></ins>
 
## <ins><b> References </b></ins>
