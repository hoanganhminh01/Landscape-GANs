<div align="center">
<figure>

 <h1> <b>Landscape Generation using Generative Adversarial Networks: A Comparison </b> </h1>
 <h3> Minh Hoang & Kenneth Ma </h3>
 
</figure>
</div>
 <em>Published on December 13, 2022</em>
<div align="right">
 
</div>

## <ins><b> Abstract </b></ins>
In this project, we explore various image generation techniques using Generative Adversarial Networks (GANs) to generate fake images of landscapes. We generated images with 3 different GANs and compared the results. We began by training an existing deep convolution GAN architecture on a Landscape image dataset from Kaggle, downsampling the dataset to 64x64 images and adapting the network to reduce overall training time. Then, we extended this DCGAN networks to handle 128x128 and 256x256 images. Since we didn't have time ot train on 256x256 images, we compared our results to 256x256 images generated by a StyleGANv3 that was pre-trained on a different landscape dataset. For an more in-depth overview of our procedure and results, see the following video summary. [INSERT HERE]().

## <ins><b> Introduction and Problem Statement </b></ins>
Landscapes have always been a source of inspiration to anyone fond of hiking and sightseeing. We wanted to see whether deep learning models trained on landscape images can generate novel landscapes that are as realistic as nature. We began by considering a few types of deep learning models including diffusions models and variational autoencoders, but after learning about Generative Adversarial Networks in class, we became interested in the idea of pitting two models against each other, and decided to explore those.

## <ins><b> Related Works </b></ins>
We found a few GANs that people were already using to generate landscapes (Landscape GAN, GANscapes). Ultimately, we decided we wanted to use a GAN that wasn’t already pre-trained on landscapes. We based our network architecture off of Natsu6767’s Deep Convolutional GAN (originally trained on celebrity faces) and trained it on the [Landscape Pictures](https://www.kaggle.com/datasets/arnaud58/landscape-pictures) dataset by arnaud58 on Kaggle. Minh had used StyleGANs in the past, and we found a [StyleGanv3](https://github.com/justinpinkney/awesome-pretrained-stylegan3) by justinpinkney on github that was trained on LHQ-256 (a 256x256 landscapes dataset).
<div align="center">
<figure>
 <img alt="sample1" src="https://raw.githubusercontent.com/hoanganhminh01/Landscape-Generation-GAN/main/data_preprocessed_256/preprocessed_256/00000000_(5).jpg"> 
 <img alt="sample2" src="https://raw.githubusercontent.com/hoanganhminh01/Landscape-Generation-GAN/main/data_preprocessed_256/preprocessed_256/00000023_(7).jpg">
 <img alt="sample3" src="https://raw.githubusercontent.com/hoanganhminh01/Landscape-Generation-GAN/main/data_preprocessed_256/preprocessed_256/00000038_(3).jpg">
 
 *Preprocessed images from the "Landscape Pictures" dataset by arnaud58*
</figure>
</div>

## <ins><b> Methodology </b></ins>
The Landscape Pictures dataset consisted of 4,319 images of landscapes without any metadata. Since the image sizes in the dataset were inconsistent, we needed to crop and resize the images into specific dimensions. We preprocessed the dataset into 3 different dimensions: 64 x 64, 128 x 128 and 256 x 256.

We reimplemented Natsu's DCGAN architecture for our initial approach. DCGAN, or Deep Convolutional GAN, is a generative adversarial network architecture. Some important features of DCGAN includes the replacement of pooling layers with strided convolutions in the discriminator and fractional-strided convolutions in the generator, the use of batch normalization layers in both the generator and the discriminator, the exclusion of connected hidden layers, and the use of Tanh and Leaky ReLU activation functions. The generator takes in a 100x1 noise vector, and the discriminator takes a 3 x 64 x 64 RGB images as the input. The final output of the network will be a 3 x 64 x 64 RGB image.
[Insert GAN architectures here]

## <ins><b> Experiments </b></ins>
<div align="center">
<figure>

 <img alt="model1" src="https://raw.githubusercontent.com/hoanganhminh01/Landscape-Generation-GAN/main/outputs/dcgan.png"> 
 
</figure>
</div>

 *Architecture of DCGAN*

### <ins><b> Model Evaluation </b></ins>
Since our experiments do not control for variables (we use different model architectures trained on different datasets for different amounts of time) the goal of our project is less about comparing specific training methods and more about conducting an exploration of various GAN networks to generate the “best-looking” landscape images possible. We acknowledge that “best-looking” is quite subjective. Therefore, we’ll showcase our results by making side-by-side comparisons between images generated by different networks and describe any patterns observed.

## <ins><b> Results </b></ins>
### <ins><b> Model Performance </b></ins>
<div align="center">
<figure>

 <img alt="loss1" src="https://raw.githubusercontent.com/hoanganhminh01/Landscape-Generation-GAN/main/outputs/loss64.png"> 
 <img alt="loss2" src="https://raw.githubusercontent.com/hoanganhminh01/Landscape-Generation-GAN/main/outputs/loss128.png">
 
</figure>
</div>

### <ins><b> Results </b></ins>

### <ins><b> Additional Examples: Pre-trained StyleGANv3 </b></ins>

## <ins><b> Conclusion and Future Works </b></ins>
 
## <ins><b> References </b></ins>
