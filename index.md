<div align="center">
<figure>

 <h1> <b>Landscape Generation using Generative Adversarial Networks: A Comparison </b> </h1>
 <h3> Minh Hoang & Kenneth Ma </h3>
 
</figure>
</div>
 <em>Published on December 13, 2022</em>
<div align="right">
 
</div>

## <ins><b> Abstract </b></ins>
In this project, we explore various image generation techniques using Generative Adversarial Networks (GANs) to generate fake images of landscapes. We generate images with 3 different GANs and compared the results. We begin by training an existing deep convolution GAN architecture on a Landscape image dataset from Kaggle, downsampling the dataset to 64x64 images and adapting the network to reduce overall training time. Then, we extend this DCGAN networks to handle 128x128 and 256x256 images. Due to our limitations when training 256x256 images, we compare our results to 256x256 images generated by a StyleGANv3 that was pre-trained on a different landscape dataset instead. For an more in-depth overview of our procedure and results, see the following video summary. [INSERT HERE]().

## <ins><b> Introduction and Problem Statement </b></ins>
Landscapes can be beautiful and inspiring - especially for those fond of hiking and sightseeing. We want to see whether deep learning models trained on landscape images can generate novel landscapes that are as realistic as nature. We began by considering a few types of deep learning models including diffusions models and variational autoencoders. After learning about Generative Adversarial Networks in class, we became interested in the idea of training two models against each other, and decided to make GANs the focus for this project.

Therefore, for our project, we decided to reimplement Radford et al.'s [paper<sup>[1]</sup>](https://arxiv.org/abs/1511.06434v2) architecture of DCGAN. We will implement DCGAN to train on a new dataset of landscapes of different sizes and see how well each model perform. Although there have been many more new models developed to solve this problem with higher quality results, , we would still choose to re-implement the models of this paper due to their simplicity, robustness but efficiency in generating new images in a fairly short amount of time, and also because this is one of the very well-known GAN models developed to solve this problem. Our code, plots and outputs can be found [here](https://github.com/hoanganhminh01/Landscape-Generation-GAN).


## <ins><b> Related Works </b></ins>
Unsupervised Representation Learning have been attracting great interests in recent years. Specifically, the generative models for images have been studied deeply and many different types of deep learning models have been used to generate new images. In addition to GANs, there are some other models that have also been used for this tasks. Some of those includes Variational Autoencoders (VAEs) and Stable Diffusion Models. Razavi et al. [<sup>[2]</sup>](https://proceedings.neurips.cc/paper/2019/file/5f8e2fa1718d1bbcadf1cd9c7a54fb8c-Paper.pdf) explored the use of Vector Quantized VAEs to generate diverse images on a larger scale that can be competitively better than GANs on multifaceted datasets like ImageNet. Gorijala and Dukkipati [<sup>[3]</sup>](https://arxiv.org/abs/1701.04568) proposed Variational InfoGAN, an extension and combination of VAE and GAN, to generate new images, as well as to modify them by fixing the latent representation of image. One of the most well-known architectures for image generation, StyleGAN by Kerras et al [<sup>[4]</sup>](https://arxiv.org/abs/1812.04948), applied the use of style transfer literature so the model can automatically learn and unsupervisedly separate high level attributes in a very efficient way.

<!---
We found a few GANs that people were already using to generate landscapes (Landscape GAN, GANscapes). Ultimately, we decided we wanted to use a GAN that wasn’t already pre-trained on landscapes. We based our network architecture off of Natsu6767’s Deep Convolutional GAN (originally trained on celebrity faces) and trained it on the [Landscape Pictures](https://www.kaggle.com/datasets/arnaud58/landscape-pictures) dataset by arnaud58 on Kaggle. Minh had used StyleGANs in the past, and we found a [StyleGanv3](https://github.com/justinpinkney/awesome-pretrained-stylegan3) by justinpinkney on github that was trained on LHQ-256 (a 256x256 landscapes dataset).
--->

## <ins><b> Methodology </b></ins>
We use a Kaggle dataset called The Landscape Pictures for our project. The Landscape Pictures dataset consists of 4,319 images of landscapes without any metadata. Since the image sizes in the dataset were inconsistent, we need to crop and resize the images into specific dimensions. We preprocess the dataset into 3 different dimensions: 64 x 64, 128 x 128 and 256 x 256.

<div align="center">
<figure>
 <img alt="sample1" src="https://raw.githubusercontent.com/hoanganhminh01/Landscape-Generation-GAN/main/data_preprocessed_256/preprocessed_256/00000000_(5).jpg"> 
 <img alt="sample2" src="https://raw.githubusercontent.com/hoanganhminh01/Landscape-Generation-GAN/main/data_preprocessed_256/preprocessed_256/00000023_(7).jpg">
 
</figure>
</div>

<div align="center">
 
  <em>Preprocessed images from the "Landscape Pictures" dataset by arnaud58</em>
 </div>

We reimplemented Natsu's DCGAN architecture for our initial approach. DCGAN, or Deep Convolutional GAN, consists of 4 transposed convolutional layers. Some additional important features of DCGAN includes the replacement of pooling layers with strided convolutions in the discriminator and fractional-strided convolutions in the generator, the use of batch normalization layers in both the generator and the discriminator, the exclusion of connected hidden layers, and the use of Tanh and Leaky ReLU activation functions. The generator takes in a 100x1 noise vector, and the discriminator takes a 3 x 64 x 64 RGB images as the input. The final output of the network will be a 3 x 64 x 64 RGB image.

<div align="center">
<figure>

 <img alt="model1" src="https://raw.githubusercontent.com/hoanganhminh01/Landscape-Generation-GAN/main/outputs/dcgan.png"> 
 
 <em>Architecture of DCGAN</em>
</figure>
</div>

## <ins><b> Experiments </b></ins>
DCGAN originally worked only for 64 x 64 images. Therefore, besides reimplementing and training the original DCGAN on our preprocessed dataset, we extended the model to be able to train on larger images. We built 2 deeper DCGAN models, DCGAN128 and DCGAN256 to take on 128 x 128 and 256 x 256 images. For each image of double size, we added 1 additional transposed convolutional layer of doubled filter size, with batch normalization and activation function, to the generator and discriminator. We also have to adjust the noise input sizes and filter sizes to accommodate the new image sizes. Since DCGAN128 and DCGAN256's architectures are much deeper and consists of much more parameters, we expect the training time to be much longer. DCGAN was originally trained on CelebA with a batch size of 128 and learning rate of 0.0002, and we will also use the same hyperparameters to train our models. Being able to train GANs to generate new images requires a very large number of epochs, but due to the limit of time and computational resources, we can only train for a smaller number of epochs compared to the original paper. Specifically, we will train DCGAN for 2000 epochs, DCGAN128 for 1000 epochs, and DCGAN256 for 500 epochs.

In addition to training our 3 DCGAN models, we also feed part of a resized LHQ dataset, now consisting of 256 x 256 images, into a pretrained StyleGANv3 network to compare the outputs.

### <ins><b> Model Evaluation </b></ins>
Since our experiments do not control for variables (we use different model architectures trained on different datasets for different amounts of time) the goal of our project is less about comparing specific training methods and more about conducting an exploration of various GAN networks to generate the “best-looking” landscape images possible. We acknowledge that “best-looking” is quite subjective. Therefore, we’ll showcase our results by making side-by-side comparisons between images generated by different networks and describe any patterns observed.

## <ins><b> Results </b></ins>
### <ins><b> Model Performance </b></ins>
<div align="center">
<figure>

 <img alt="loss1" src="https://raw.githubusercontent.com/hoanganhminh01/Landscape-Generation-GAN/main/outputs/loss64.png"> 
 <img alt="loss2" src="https://raw.githubusercontent.com/hoanganhminh01/Landscape-Generation-GAN/main/outputs/loss128.png">
 
</figure>
</div>

For the original DCGAN model, training 2000 epochs takes about 2.5 hours on CUDA with RTX 3060 GPU (average of 3 - 5 seconds per epoch) and about 8 hours on CPU. For DCGAN128, training 1000 epochs takes about 8 hours on GPU (average of 27 - 30 seconds per epoch). For DCGAN256, we were unable to train our model due to CUDA running out of memory when feeding input through the network. Therefore, we could not generate any plot and result for our DCGAN256 experiment.

For the other 2 models, with the same learning rate and batch size, we can look at the plots and see that overall, the losses of DCGAN128 is smaller and less fluctuating that those of DCGAN. The generator's loss of both models both reach approximately 25 in the first 35000 iterations, but way less often in DCGAN128. Similarly, the discriminator's loss of both reach maximum of 15, but less often for DCGAN128.

### <ins><b> Results </b></ins>

#### <ins><b> Comparisons </b></ins>
<div align="center">
<figure>
 <img alt="result1" src="https://raw.githubusercontent.com/hoanganhminh01/Landscape-Generation-GAN/main/outputs/image64.png"> 
 <img alt="result2" src="https://raw.githubusercontent.com/hoanganhminh01/Landscape-Generation-GAN/main/outputs/image128.png"> 
 
 <em>Comparisons between real and fake images generated by DCGAN models</em>
</figure>
</div>

Overall, the generated results from the DCGAN128 are more realistic. Several of them look like beach landscapes, with blue-gray skies, beige sandbars and darker brown mountain ranges in-between. It seems the model has gotten quite good at generating these kinds of landscapes, where the different features organized into horizontal layers. If these images were inputs for an image segmentation task, they would likely be fairly easy to predict. We can also observe another archetype of generated images that are covered in green splotches. In particular, the image in row 4 column 2 looks like a waterfall of some sort. It's possible that the green splotches are supposed to be trees and shrubbery of some sort, which would make sense against a waterfall. However, such landscapes are segmented into many more regions than the aforementioned beach landscapes. These would be harder inputs for an image segmentation task, and this could also be why the model has a harder time generating realistic versions. Finally, we also see a few images with orange skies. It's possible that this pattern was learned from sunset samples like row 2 column 6 of "real images".

The generated results of DCGAN for 64 x 64 images are still quite blurry, although we are able to partly visualize landscapes, while those from the DCGAN128 are more realistic, but some are still hard to visualize. This explains more why the loss plot of DCGAN is more fluctuating than DCGAN128, even though it was trained for 1000 epochs. Given the details of landscape images, it's harder to learn the important features of the whole landscape when the dimension of the images is too small. Compared to CelebA and MNIST, the datasets that DCGAN was originally trained on, the variance of landscapes are much higher due to the main features are not centered and the fact that there are a lot of factors making up of a landscape. Therefore, by having an additional layer to learn features from larger images, DCGAN128 was able to generated more realistic outputs in less epochs.


#### <ins><b> Animation: Fake Image Generation </b></ins>

<div align="center">
<figure>
 <img alt="result3" src="https://github.com/hoanganhminh01/Landscape-Generation-GAN/blob/main/outputs/animation64.gif?raw=true"> 
 <img alt="result4" src="https://github.com/hoanganhminh01/Landscape-Generation-GAN/blob/main/outputs/animation128.gif?raw=true">
 
 
 <em>Fake image evolution of DCGAN models</em>
</figure>
</div>

The animation shows that DCGAN learns important features slower the DCGAN128, being able to generate images, although still blurry sometimes, containing important edges constructing a proper landscape. 

<div align="center">
<figure>
 <img alt="result5" src="https://github.com/hoanganhminh01/Landscape-Generation-GAN/blob/main/outputs/stylegan3.gif?raw=true"> 
 
</figure>
</div>

<div align="center">
 
 <em>Sample of generated 256 x 256 image of StyleGANv3</em>
 </div>

The animation clearly shows how superios StyleGANv3 is compared to DCGAN in generating realistic outputs. This is expected, since StyleGANv3 were pre-trained on multiple GPUs for several days (average of 72.7s / 1000 images) on a much larger dataset (LHQ dataset), so it is common to generate very good results.
## <ins><b> Conclusion and Future Works </b></ins>

During this project, we reimplemented and extended the architecture of DCGAN to work with multiple different sizes of images. We leanred that in order to train larger images, a deeper DCGAN network is needed, and the deeper the network is, the better it is a generating realistic outputs. 

We also noticed that the generated outputs are not very realistic yet and still need improvements, compared to the output we got when testing on a pretrained StyleGANv3. This is due to the limit of time and we were only able to train for a fairly small number of epochs. We were unable to train out model on 256 x 256 images due to computational limitations. Additionally, our dataset is fairly small, so it may not be enough to generate images that can transition smoothly between different landscapes like the output of StyleGANv3, which was trained only a much larger dataset - LHQ. Therefore, if we were to continue our project, with enough computational resources of multiple GPUs and time, we would train our DCGAN models on the full LHQ dataset for a fairly large number of epochs (50000 to 80000 epochs) to see how realistic our output can be.
 
## <ins><b> References </b></ins>
[1] Alec Radford, Luke Metz, Soumith Chintala. Unsupervised Representation Learning with Deep Convolutional Generative Adversarial Networks. ICLR (Poster) 2016.

[2] Ali Razavi, Aäron van den Oord, and Oriol Vinyals. 2019. Generating diverse high-fidelity images with VQ-VAE-2. Proceedings of the 33rd International Conference on Neural Information Processing Systems. Curran Associates Inc., Red Hook, NY, USA, Article 1331, 14866–14876.

[3] Mahesh Gorijala and Ambedkar Dukkipati. “Image Generation and Editing with Variational Info Generative AdversarialNetworks.” ArXiv abs/1701.04568 (2017).

[4] T. Karras, S. Laine and T. Aila, "A Style-Based Generator Architecture for Generative Adversarial Networks" in IEEE Transactions on Pattern Analysis & Machine Intelligence, vol. 43, no. 12, pp. 4217-4228, 2021. doi: 10.1109/TPAMI.2020.2970919.
